{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "talented-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required packages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "naval-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "  \"weights_path\"    : \"/Users/rishitdholakia/Desktop/Car Damage Detection project/Car Damage check/model0.2.h5\",\n",
    "  \"features_path\"   : \"/Users/rishitdholakia/Desktop/Car Damage Detection project/Car Damage check/features.h5\",\n",
    "  \"labels_path\"     : \"/Users/rishitdholakia/Desktop/Car Damage Detection project/Car Damage check/labels.h5\",\n",
    "  \"classifier_path\" : \"/Users/rishitdholakia/Desktop/Car Damage Detection project/Car Damage check/classifier.pickle\",\n",
    "  \"model_path\"      : \"/Users/rishitdholakia/Desktop/Car Damage Detection project/Car Damage check/model0.2.json\",\n",
    "\n",
    "  \"test_size\"       : 0.20,\n",
    "  \"seed\"            : 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "official-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "test_size     = config[\"test_size\"]\n",
    "seed      = config[\"seed\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "classifier_path = config[\"classifier_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sound-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import features and labels\n",
    "h5f_data  = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset_1']\n",
    "labels_string   = h5f_label['dataset_1']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acute-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] features shape: (1840, 4096)\n",
      "[INFO] labels shape: (1840,)\n",
      "[INFO] training started...\n",
      "[INFO] splitted train and test data...\n",
      "[INFO] train data  : (1472, 4096)\n",
      "[INFO] test data   : (368, 4096)\n",
      "[INFO] train labels: (1472,)\n",
      "[INFO] test labels : (368,)\n"
     ]
    }
   ],
   "source": [
    "# verify the shape of features and labels\n",
    "print (\"[INFO] features shape: {}\".format(features.shape))\n",
    "print (\"[INFO] labels shape: {}\".format(labels.shape))\n",
    "\n",
    "print (\"[INFO] training started...\")\n",
    "# split the training and testing data\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(features,\n",
    "                                                                  labels,\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=seed)\n",
    "\n",
    "print (\"[INFO] splitted train and test data...\")\n",
    "print (\"[INFO] train data  : {}\".format(trainData.shape))\n",
    "print (\"[INFO] test data   : {}\".format(testData.shape))\n",
    "print (\"[INFO] train labels: {}\".format(trainLabels.shape))\n",
    "print (\"[INFO] test labels : {}\".format(testLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deluxe-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/newenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use logistic regression as the model\n",
    "print (\"[INFO] creating model...\")\n",
    "model = LogisticRegression(random_state=seed)\n",
    "model.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distinct-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model of test data\n",
    "preds = model.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "organizational-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
      " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
      " 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0\n",
      " 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "scientific-seven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving model...\n",
      "[INFO] confusion matrix\n",
      "0.9021739130434783\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# dump classifier to file\n",
    "print (\"[INFO] saving model...\")\n",
    "pickle.dump(model, open(classifier_path, 'wb'))\n",
    "\n",
    "# display the confusion matrix\n",
    "print (\"[INFO] confusion matrix\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "cm = confusion_matrix(testLabels, preds)\n",
    "\n",
    "score = model.score(testData, testLabels)\n",
    "print(score)\n",
    "score = model.score(trainData, trainLabels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-mattress",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
